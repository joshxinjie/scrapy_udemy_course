(base) C:\Users\Joshl\OneDrive\Documents>scrapy shell
2019-02-06 17:34:48 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: scrapybot)
2019-02-06 17:34:48 [scrapy.utils.log] INFO: Versions: lxml 4.2.3.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.7.0, Python 3.6.6 |Anaconda custom (64-bit)| (default, Jun 28 2018, 11:27:44) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1a  20 Nov 2018), cryptography 2.4.2, Platform Windows-10-10.0.17134-SP0
2019-02-06 17:34:48 [scrapy.crawler] INFO: Overridden settings: {'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0}
2019-02-06 17:34:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2019-02-06 17:34:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-02-06 17:34:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-02-06 17:34:48 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-02-06 17:34:48 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    <scrapy.crawler.Crawler object at 0x00000257E7B2AC18>
[s]   item       {}
[s]   settings   <scrapy.settings.Settings object at 0x00000257EA289940>
[s] Useful shortcuts:
[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)
[s]   fetch(req)                  Fetch a scrapy.Request and update local objects
[s]   shelp()           Shell help (print this help)
[s]   view(response)    View response in a browser
In [1]: from scrapy.selector import Selector

In [2]: html_doc = '''<html>
   ...:   <head>
   ...:     <title>Title of the page</title>
   ...:   </head>
   ...:   <body>
   ...:     <h1>H1 Tag</h1>
   ...:     <h2>H2 Tag with <a href="#">link</a></h2>
   ...:     <p>First Paragraph</p>
   ...:     <p>Second Paragraph</p>
   ...:   </body>
   ...: </html>
   ...: '''

In [3]: sel = Selector(text=html_doc)

In [4]: sel.extract()
Out[4]: '<html>\n  <head>\n    <title>Title of the page</title>\n  </head>\n  <body>\n    <h1>H1 Tag</h1>\n    <h2>H2 Tag with <a href="#">link</a></h2>\n    <p>First Paragraph</p>\n    <p>Second Paragraph</p>\n  </body>\n</html>'

In [5]: sel.xpath('/html/head/title')
Out[5]: [<Selector xpath='/html/head/title' data='<title>Title of the page</title>'>]

In [6]: sel.xpath('/html/head/title').extract()
Out[6]: ['<title>Title of the page</title>']

In [7]: sel.xpath('//title').extract()
Out[7]: ['<title>Title of the page</title>']

In [8]: sel.xpath('//text()').extract()
Out[8]:
['\n  ',
 '\n    ',
 'Title of the page',
 '\n  ',
 '\n  ',
 '\n    ',
 'H1 Tag',
 '\n    ',
 'H2 Tag with ',
 'link',
 '\n    ',
 'First Paragraph',
 '\n    ',
 'Second Paragraph',
 '\n  ',
 '\n']

In [9]: sel.xpath('/html/body/p')
Out[9]:
[<Selector xpath='/html/body/p' data='<p>First Paragraph</p>'>,
 <Selector xpath='/html/body/p' data='<p>Second Paragraph</p>'>]

In [10]: sel.xpath('/html/body/p').extract()
Out[10]: ['<p>First Paragraph</p>', '<p>Second Paragraph</p>']

In [11]: sel.xpath('//p').extract()
Out[11]: ['<p>First Paragraph</p>', '<p>Second Paragraph</p>']

In [12]: sel.xpath('//p[1]').extract()
Out[12]: ['<p>First Paragraph</p>']

In [13]: sel.xpath('//p[2]').extract()
Out[13]: ['<p>Second Paragraph</p>']

In [14]: sel.xpath('//p')[0].extract()
Out[14]: '<p>First Paragraph</p>'

In [15]: sel.xpath('//p')[1].extract()
Out[15]: '<p>Second Paragraph</p>'

In [16]: sel.xpath('//p/text()')[0].extract()
Out[16]: 'First Paragraph'

In [17]: sel.xpath('//p/text()').extract_first()
Out[17]: 'First Paragraph'

In [18]: sel.xpath('//h2')
Out[18]: [<Selector xpath='//h2' data='<h2>H2 Tag with <a href="#">link</a></h2'>]

In [19]: sel.xpath('//h2').extract()
Out[19]: ['<h2>H2 Tag with <a href="#">link</a></h2>']

In [20]: sel.xpath('//h2/a').extract()
Out[20]: ['<a href="#">link</a>']

In [21]: sel.xpath('//h2/a/@href').extract()
Out[21]: ['#']

In [22]: sel.css('h2')
Out[22]: [<Selector xpath='descendant-or-self::h2' data='<h2>H2 Tag with <a href="#">link</a></h2'>]